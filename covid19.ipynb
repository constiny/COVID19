{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Timeline\n",
    "    - Monday: Data acquiration \n",
    "    - Tuesday: Pineline, EDA and Visualization\n",
    "    - Wednesday: Hypotheses testing and get things organized\n",
    "    - Thursday: Preparation for presentation\n",
    "    - Friday: Update latest data and make minor revise\n",
    "\n",
    "-----------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monday: Data acquiration\n",
    "    - [x] Getting CSSE data on github\n",
    "    - [x] Gather state testing data thru API\n",
    "    - [x]  Webscrap korean testing data\n",
    "    - [x]  Webscrap LA community level data\n",
    "    - []  consolidate as database backup into psql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Source:**\n",
    "\n",
    "- **Getting CSSE data on github**\n",
    "\n",
    "    source: https://github.com/CSSEGISandData/COVID-19 by Johns Hopkins CSSE\n",
    "\n",
    "\n",
    "- **Gather state testing data thru API**\n",
    "\n",
    "    source: https://covidtracking.com/data by Johns Hopkins CSSE\n",
    "    - US Testing in Time Series https://covidtracking.com/api/us/daily.csv\n",
    "    - States Historical Data https://covidtracking.com/api/states/daily.csv\n",
    "\n",
    "\n",
    "- **Webscrap korean testing data**\n",
    "\n",
    "    Source: CDC of South Korean\n",
    "    - sample data https://www.cdc.go.kr/board/board.es?mid=&bid=0030&act=view&list_no=366735\n",
    "    \n",
    "    \n",
    "- **Webscrap LA community level data**\n",
    "\n",
    "    Source: The Department of Public Health of Los Angeles County\n",
    "    - sample data http://publichealth.lacounty.gov/phcommon/public/media/mediapubhpdetail.cfm?prid=2298"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Web scraping preparion**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, create a webscrap class including functions for getting tables, get text and get urls. Such class definition is stored as ./src/web_scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T21:20:48.551804Z",
     "start_time": "2020-04-06T21:20:48.374997Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.web_scaping import webscrap\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gather data with webscape class**\n",
    "\n",
    "Many experts treated South Korea as a best practice in controlling COVID-19 and an good model to follow. Hence, in this study we will compare COVID-19 data between US and South Korea.\n",
    "\n",
    "Unfortunately, there isn't an existing data source for South Korea testing data for the coronavirus. We utilized web scaping technique to collect data from press release of each day.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather all urls for all korean press releases\n",
    "\n",
    "url_p1 = \"https://www.cdc.go.kr/board.es?mid=&bid=0030&nPage=\"\n",
    "url_p2 = (str(i) for i in list(range(1,25)))\n",
    "\n",
    "url_lst = []\n",
    "\n",
    "for page in url_p2:\n",
    "    nws = webscrap(url_p1 + page)\n",
    "    nws.start(random_headers = True)\n",
    "    url_lst.append(nws.get_urls())\n",
    "\n",
    "with open(\"url_k_t.csv\", \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(url_lst2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather Table from each page\n",
    "\n",
    "url_p1 = \"https://www.cdc.go.kr\"\n",
    "out_lst = []\n",
    "\n",
    "for page in urls2:\n",
    "    nws = webscrap(url_p1 + page)\n",
    "    nws.start(random_headers = True)\n",
    "    out_lst.append(nws.get_table())\n",
    "\n",
    "with open(\"South_Korea_History data.csv\", \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(out_lst)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not only the big picture in a country scale that we care, we also wants to provide a LA picture community level data. Similarly, we webscaped community level data from press release from Department of Public Health of Los Angeles County."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect LA data\n",
    "\n",
    "url = \"http://publichealth.lacounty.gov/phcommon/public/media/mediapubhpdetail.cfm?prid=\"\n",
    "url_lst = []\n",
    "for i in list(range(2300,2267,-1)):\n",
    "    url_lst.append(url + str(i))\n",
    "out = []\n",
    "for url in url_lst:\n",
    "    nws = webscrap(url)\n",
    "    nws.start(random_headers = True)\n",
    "    temp_lst = []\n",
    "    text = nws.get_text(elements = [\"ul\", \"li\"])\n",
    "    if text:\n",
    "        temp_lst.append(url[-4:])\n",
    "        temp_lst.append(nws.get_table(index=1)[0][0])\n",
    "        temp_lst.append(text)\n",
    "        out.append(temp_lst)\n",
    "\n",
    "# Save as csv\n",
    "\n",
    "with open(\"LA_backup.csv\", \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "\n",
    "Create a data pipeline to structure data\n",
    "- LA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lacounty_daily_text_2_table(text_list):\n",
    "    # pipeline function for date from mar 30\n",
    "    out_lst = []\n",
    "    temp = list(filter(lambda x: \"City\" in x, text_list))\n",
    "    if temp:\n",
    "        string = temp[0]\n",
    "        str_lst2 = string.split(\"\\t  \")\n",
    "        string2 = str_lst2[0][:str_lst2[0].find(\"\\t(\\t\")]\n",
    "\n",
    "        for i in str_lst2:\n",
    "            string2 = i[:i.find(\"\\t(\\t\")]\n",
    "            out_lst.append(string2.replace(\"\\t\",\",\").split(\",\"))\n",
    "    return out_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lacounty_daily_text_2_table2(text_list):\n",
    "    # pipeline function for date of mar 22 - mar 29\n",
    "    out_lst = []\n",
    "    temp = list(filter(lambda x: \"City\" in x, text_list))\n",
    "    if temp:\n",
    "        string = temp[0]\n",
    "        str_lst2 = string.split(\"\\t\")\n",
    "        for i in range(len(str_lst2) - 1):\n",
    "            temp_lst = []\n",
    "            temp_lst.append (str_lst2[i].split(\"  \", 1)[1])\n",
    "            temp_lst.append (str_lst2[i + 1].split(\"  \", 1)[0])\n",
    "            out_lst.append(temp_lst)\n",
    "    return out_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data pipeline after Mar 30\n",
    "out2 = []\n",
    "for item in out:\n",
    "    temp_lst = [item[1].split(\"\\r\\n\\t\")[1]]\n",
    "    if len(item[2]) > 0:\n",
    "        temp_lst.append(lacounty_daily_text_2_table(item[2]))\n",
    "    out2.append(temp_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data pipeline before mar 29\n",
    "out3 = []\n",
    "for item in out[11:]:\n",
    "    temp_lst = [item[1].split(\"\\r\\n\\t\")[1]]\n",
    "    if len(item[2]) > 0:\n",
    "        temp_lst.append(lacounty_daily_text_2_table2(item[2]))\n",
    "    out3.append(temp_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LA_data = {}\n",
    "for i in out2[:11]:\n",
    "    if len(i[1]):\n",
    "        LA_data[i[0]] = i[1]\n",
    "for i in out3:\n",
    "    if len(i[1]):\n",
    "        LA_data[i[0]] = i[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge into a readable format\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(LA_data['April 06, 2020'], columns= [\"city\", 'April 06, 2020'] )\n",
    "df.set_index([\"city\"])\n",
    "\n",
    "for key in LA_data.keys():\n",
    "    df2 = pd.DataFrame(LA_data[key], columns= [\"city\", key])\n",
    "    df2.set_index([\"city\"])\n",
    "    df = pd.merge(df, df2, how='left', left_on=\"city\", right_on=\"city\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After checking the merged dataset, some data was not properly merge since the city name format has been change since March 26, 2020. We need do minor fix on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([\"April 06, 2020_y\", \"March 26, 2020\", \"March 25, 2020\", \"March 24, 2020\", \"March 23, 2020\", \"March 22, 2020\"], axis=1, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to fix city names\n",
    "def aka_name(city_name):\n",
    "    new_name = \"\"\n",
    "    if city_name[:7] == \"City of\":\n",
    "        new_name += city_name[-(len(city_name) - 8) : ]\n",
    "    elif city_name[:14] == 'Unincorporated':\n",
    "        new_name += city_name[-(len(city_name) - 17) : ]\n",
    "    elif city_name[:14] == 'Los Angeles - ':\n",
    "        new_name += city_name[-(len(city_name) - 14) : ]\n",
    "    else:\n",
    "        new_name += city_name\n",
    "    return new_name\n",
    "\n",
    "df[\"city2\"] = df[\"city\"].apply(lambda x: aka_name(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix data for the first 5 day data\n",
    "key_list = list(LA_data.keys())[-5 : ]\n",
    "for key in key_list:\n",
    "    df2 = pd.DataFrame(LA_data[key], columns= [\"city2\", key])\n",
    "    df2[\"city2\"] = df2[\"city2\"].apply(lambda x : x.strip(\"*\"))\n",
    "    df2.set_index([\"city2\"])\n",
    "    df = pd.merge(df, df2, how='left', left_on=\"city2\", right_on=\"city2\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a csv backup\n",
    "df.to_csv(\"la_community_0406.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get korea data into Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T05:32:12.189226Z",
     "start_time": "2020-04-07T05:32:12.183895Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"./Data/Testing/South_Korea_History data.csv\", \"r\", newline=\"\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    k_data = list(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T06:37:57.749274Z",
     "start_time": "2020-04-07T06:37:57.732678Z"
    }
   },
   "outputs": [],
   "source": [
    "tmp_lst = list(i[2] for i in k_data)\n",
    "tmp_lst2 = []\n",
    "for item in tmp_lst:\n",
    "    tmp_lst3 = item.replace(', ', '').replace(\"'\", '').replace(\"]\", '').split(\"\\\\n\")[1:]\n",
    "    tmp_lst2.append(tmp_lst3)\n",
    "    \n",
    "for i in range(len(tmp_lst2)):\n",
    "    for j in range(len(tmp_lst2[i])):\n",
    "        tmp_lst2[i][j] = tmp_lst2[i][j].replace(\"As of 0:00\", '').replace(\"As of 9:00\", '').replace(\"As of 16:00\", '').replace(\"As of 09:00\", '')\n",
    "    if len(tmp_lst2[i][0].strip()) == 0:\n",
    "        tmp_lst2[i] = tmp_lst2[i][1:]\n",
    "    if  \" (\" in tmp_lst2[i][0]:\n",
    "        tmp_lst2[i][0] = tmp_lst2[i][0][: tmp_lst2[i][0].find(\" (\")]        \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T17:48:54.122487Z",
     "start_time": "2020-04-06T17:48:54.118833Z"
    }
   },
   "source": [
    "# Tuesday: Pineline, EDA and Visualization\n",
    "    -[ ] Data cleaning and get ready for Pandas\n",
    "    -[ ] Data screening and mark outlier\n",
    "    -[ ] create pineline to process data\n",
    "    -[ ] EDA on confirm case, death case, daily confirm case and testing data on Korean, CA, NY, US\n",
    "    -[ ] Visualization on community level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T05:51:33.665784Z",
     "start_time": "2020-04-07T05:51:33.659402Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[',\n",
       " 'As of 0:00',\n",
       " '5 April (Sun)',\n",
       " '461,233',\n",
       " '10,237',\n",
       " '6,463',\n",
       " '3,591',\n",
       " '183',\n",
       " '19,571',\n",
       " '431,425']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_lst[0].replace(', ', '').replace(\"'\", '').replace(\"]\", '').split(\"\\\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T06:39:59.798681Z",
     "start_time": "2020-04-07T06:39:59.789738Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(tmp_lst2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T06:39:39.140492Z",
     "start_time": "2020-04-07T06:39:38.995402Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T06:41:11.725013Z",
     "start_time": "2020-04-07T06:41:11.718992Z"
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"k_data_0406.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
